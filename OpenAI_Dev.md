# API Key Setup
The OpenAI key for our project has been set up. It's crucial that we don't reveal the key in any public places, including in pushed (and therefore publically available) code. The key can be found in a [Google Doc](https://docs.google.com/document/d/1QfPruCe_L83czCVAeWtUW0drx1H_ROMUNqtmXXvlvRs/edit?usp=drive_link) in our shared project folder.
## Billing Limits
For the purpose of project development, I've added credits to the platform that will automatically top up every time it goes below a certain level. There's currently a $100/month usage limit, which should be sufficient for this project however this can be increased if needed. 
## Usage Limits
With model `gpt-3.5-turbo`, we have 60,000 tokens per month in token limits. 100 tokens equates to about 75 words outputted by the model. Therefore, we can receive about 45,000 words in summarization outputs over the course of developing the project. Should this not be enough, we can have someone else in the team create another API key, or use an older model with a higher token limit. Request limits should not be an issue, as we have 10,000 requests per day (each call to the API is a request).
# Parameters
## Model
Given prior usage of the OpenAI API platform, we're on a usage tier that provides us with access to the `gpt-4` model. This will be super useful for producing the best outputs. However, we will likely need to make the `model` a variable the user selects. This is because we are only deploying in the front end, and so the user will enter their own OpenAI API key (so that our API key is not publically available). Some users may not have access to some models, and therefore we cannot hardcode which model the API call uses. Also, different models have different billing rates and usage limits, so to decrease the chance the user has any API issues on their end, the model selection should be a choice made by a user. This can be a drop down menu where the user selects from pre-defined options (e.g., `gpt-4`, `gpt-3.5-turbo`, etc.), potentially in a small "Settings" icon in the top left of the UI.
## Temperature
The `temperature` is a parameter that controls the randomness of the response. Lower temperature means the response is more deterministic and consistent. For summarization tasks, to ensure accuracy, we should have a lower temperature setting of around `0.3`. This will be a hard-coded variable. As we continue to prompt engineer for different response types, we can play around with this parameter and potentially have different `temperature` values for different summarization tones.
## Maximum Length
The `max_tokens` parameter determines the length of the output generated by the model. As mentioned, 100 tokens equates to about 75 words. Therefore, like `temperature`, this can be a harcoded variable that is different for each summarization type. More complex summaries will have a higher `max_tokens` value.
# Development
## Prompt Engineering
Prompt engineering is being conducted in the [OpenAI Playground](https://platform.openai.com/playground) where prompts can rapidly be tested while editing the text prompt and parameter settings. Once a suitable prompt is found, this can easily be exported directly from the Playground into formatted code (either in `python`, `node.js`, or `json`). This will make implementing the summarization code into the front-end very quick and easy.
## Next Steps
Once all the different types of pre-set summarization options are determined (e.g. tone, length, style), we can finish prompt engineering. We will continue looking into best practices and techniques for summarization prompt techniques. For now, testing is being done on a range of dummy texts that cover different content the user may use the extension on. This can be news articles, Wikipedia pages, or API docs for instance. Once prompt engineering is completed in the Playground, it can be rapidly implemented in the front-end. 
